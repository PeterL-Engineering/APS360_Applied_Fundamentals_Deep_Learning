\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
%\newcommand{\apsname}{Project Proposal}
\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{40}

\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{url}
\usepackage{graphicx}

%######## APS360: Put your project Title here
\title{Image Colourization via Convolutional \\
Neural Networks and Deep Learning}


%######## APS360: Put your names, student IDs and Emails here
\author{Youssef Fikry  \\
Student\# 1005678901\\
\texttt{youssef.fikry@mail.utoronto.ca} \\
\And Harkirpa Kaur  \\
Student\# 1011242479 \\
\texttt{harkirpa.kaur@mail.utoronto.ca} \\
\AND
Peter Leong \\
Student\# 1005678901 \\
\texttt{peter.leong@mail.utoronto.ca} \\
\And
Thulasi Thavarajah \\
Student\# 10115358424 \\
\texttt{t.thavarajah@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
This project addresses the challenge of automated colourization for 256$\times$256 grayscale images using a dataset of 12,600 image pairs, balanced across human subjects, 
animals, and natural scenery. We frame colourization as a supervised learning problem in the CIELAB colour space, where a model predicts chrominance channels ($a^*$, $b^*$) 
from the luminance channel ($L^*$). A shallow convolutional neural network (CNN) provides the baseline performance, while our primary solution employs a deeper convolutional encoder-decoder architecture. This 
design captures high-level semantic features and spatial context, addressing limitations of shallow networks in perceptual realism. All source code, datasets, and results 
are publicly available \href{https://drive.google.com/drive/folders/1cV1NhlQ8UTk_CgJdwhqeRu0z5xE85ZsI?usp=sharing}{here}. 
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}


\section{Introduction}

While colour photography processes first emerged in the 1890s, widespread accessibility was not achieved until the 1970s \citep{scienceandmediamuseum2020}. Consequently, most historic
photographs exist only in black and white, lacking the visual richness expected by modern audiences. For individuals with vision impairments---such as those who have undergone cataract 
surgery---interpreting grayscale images poses significant challenges \citep{vogelsang2024impact}, rendering much of photographic history inaccessible to them. 

This project leverages deep learning to automate the colourization of black-and-white images, with the dual goals of restoring lost visual information and enhancing accessibility. 
Traditional colourization methods often yield desaturated results and require laborious manual intervention \citep{cheng2016deepcolorization}, whereas convolutional neural networks (CNNs) 
learn spatial and semantic features to generate realistic colours autonomously \citep{zhang2016colorful}. By automating this process, we aim to make historical imagery more engaging and inclusive.

\section{Inidividual Contributions & Responisbilites}

\section{Baseline Model}

To establish a performance benchmark for our colorization task, we implemented a shallow convolutional neural network called \texttt{ShallowColorizationNet}. This model uses a simple 
encoder-bottleneck-decoder architecture and does not incorporate adversarial training. Its purpose is to test the feasibility of learning a direct mapping from grayscale luminance (\textit{L}) 
input to the chrominance (\textit{ab}) channels in the CIELAB color space.

\subsection{Model Architecture}

The architecture of the baseline model is as follows:
\begin{itemize}
    \item \textbf{Encoder:} Two convolutional layers with ReLU activation followed by max pooling, reducing spatial dimensions while increasing channel depth.
    \item \textbf{Bottleneck:} A single convolutional layer that expands the feature representation.
    \item \textbf{Decoder:} A transposed convolution to restore spatial resolution and a final convolutional layer outputting two channels (for \textit{a} and \textit{b}), with a $\tanh$ 
    activation to constrain values between $[-1, 1]$.
\end{itemize}

\subsection{Training and Comparison Approach}

The baseline model was trained using the L1 loss between predicted and ground truth \textit{ab} channels. This model was compared to our primary neural network—a conditional 
GAN model that includes a discriminator to encourage more realistic colorizations—based on both quantitative and qualitative results.

\subsection{Quantitative and Qualitative Results}

\textbf{Quantitative:} The baseline model was trained for 5 epochs using L1 loss. The generator's loss remained stable around \textbf{1.5647}, while the discriminator's loss 
converged to approximately \textbf{0.7074}. These consistent values suggest that the baseline network was able to learn a basic mapping from grayscale to chrominance, but likely 
lacked the capacity or incentive to produce high-fidelity or diverse outputs. These values serve as reference points for evaluating improvements made by our primary GAN-based model.

\textbf{Qualitative:} The baseline model produced smooth but somewhat desaturated colorizations. It tended to predict average colors in ambiguous regions, resulting in low color 
diversity. See Figure~\ref{fig:baseline_outputs} for representative examples.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figs/baseline_output.png}
    \caption{Sample output from the baseline model on grayscale input images.}
    \label{fig:baseline_outputs}
\end{figure}

\subsection{Challenges}

The primary challenge encountered during development was the tendency of the baseline model to learn overly conservative color predictions. Without a discriminator or explicit 
diversity loss, the network favored colorizations close to the dataset mean. Additionally, selecting the appropriate normalization strategy for LAB space and constraining output to 
valid chrominance ranges required careful tuning.

\subsection{Feasibility Assessment}

Despite its simplicity, the baseline model demonstrated that the colorization task is learnable with a low-capacity network, although the outputs lacked the vividness and fidelity 
achieved by our primary GAN-based model. The baseline thus served its role in confirming the viability of end-to-end colorization from grayscale input and establishing a benchmark 
for model improvement.

\section{Data Processing}

\section{Baseline Model}

\section{Primary Model}
\label{last_page}

\newpage
\bibliographystyle{iclr2022_conference}
\bibliography{APS360_Report_ref}

\end{document}
