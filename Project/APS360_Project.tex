\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{40}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

%######## APS360: Put your project Title here
\title{Image Colourization via Convolutional \\
Neural Networks and Deep Learning}


%######## APS360: Put your names, student IDs and Emails here
\author{Youssef Fikry  \\
Student\# 1006682626\\
\texttt{youssef.fikry@mail.utoronto.ca} \\
\And
Harkirpa Kaur  \\
Student\# 1011242479 \\
\texttt{harkirpa.kaur@mail.utoronto.ca} \\
\AND
Peter Leong \\
Student\# 1005678901 \\
\texttt{peter.leong@mail.utoronto.ca} \\
\And
Thulasi Thavarajah \\
Student\# 10115358424 \\
\texttt{t.thavarajah@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
This template should be used for all your project related reports in APS360 course. -- Write an abstract for your project here. Please review the \textbf{ First Course Tutorial} for a quick start
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}



\section{Introduction}

While the colour photography processes first emerged in the 1890s, colour photography did not become widely accessible until the 1970s \cite{scienceandmediamuseum2020}. As a result, 
most historic photographs are black and white and lack the visual richness that modern viewers are accustomed to. In addition, individuals who have their cataracts 
removed as a part of vision restoration processes have shown to struggle with identifying objects in black and white images \cite{vogelsang2024impact}, making most historic photographs 
inaccessible to them. This project aims to use deep learning to automatically colorize black and white images, with the goal of restoring visual information and 
making historical imagery more accessible for all audiences. Traditional, non deep-learning based colorization methods often produce desaturated results and rely 
heavily on human guidance \cite{cheng2016deepcolorization}, making them non-scalable. Conversely, deep networks such as CNNs effectively capture spatial and semantic features and produce 
realistic colourized images without user interaction \cite{zhang2016colorful}, making deep learning an ideal approach for image colorization.

\subsection{Background \& Related Work}

The challenge of image colourization has been approached in a wide variety of ways. Even within solutions involving deep learning
techniques, there numerous unique design choices. One grouping method \cite{zeger2021grayscale} results in five categories: simple colourization neural networks,
user-guided colourization neural networks, diverse colourization neural networks, multi-path colourization neural networks, and examplar-based
neural networks. 

Simple colourization neural networks use feedforward CNNs to map grayscale inputs to colour outputs. One of the foremost solutions proposed by
Zhang et al. \cite{zhang2016colorful} used a fully convolutional network to predict the a and b channels of the CIELAB colour space from grayscale images. Their 
architecture is composed of several convulational layers, each followed by a ReLU activation function and a batch normailization layer.

User-guided colourization neural networks use user input to guide the colourization process. One such solution \cite{zhang2017real} uses a
fully convolutional network to predict the a and b channels of the CIELAB colour space from grayscale images, but also takes user input in the form of
user-provided colour scribbles. The network is trained to minimize the difference between the predicted and user-provided colours, allowing it to
learn to colourize images in a way that is consistent with the user's input. 

Diverse colourization networks produce multiple colourization outputs for a given grayscale input. One such solution \cite{Vitoria2020ChromaGAN} uses
a generative adversarial network (GAN) to produce multiple colourization outputs for a given grayscale input. The GAN is trained to minimize the difference
between the predicted and ground truth colours, allowing it to learn to produce diverse colourization outputs. 

Multi-path colourization neural networks differentiate features at different scales. Iizuka et al. \cite{Iizuka2016Colourization} proposed a multi-path
colourization neural network that uses multiple convolutional layers to extract features at different scales. The network is trained to minimize the difference
between the predicted and ground truth colours, allowing it to learn to produce colourization outputs that are consistent with the features at different scales. 

Exemplar-based neural networks use a set of exemplar images to guide the colourization process. In Su et al \cite{su2020instanceawareimagecolorization}, example images are
used to trasnfer the colour to the target image. Each instance is ouput to two different colourization networks which fuse to yield the final result. This group of 
solutions is easier to implement, as learning to colourize instances is significantly easier than learning to colourize an entire image. 

\section{Methodology}
\label{methodology}

\subsection{Data Processing}

The project's image dataset was compiled using individual datasets from Kaggle.com, an online platform that provides access to real-world datasets and a community for data scientists. To train a model that can generalize to a broader range of images, the final dataset for this project is comprised of three categories: human, animal, and scenic. All datasets chosen have a public domain license.  

The human image dataset was chosen because its original purpose is for human detection \cite[]{kaggle_human}. As a result, the dataset contains a diverse range of 17,300 images of people in different environments. 

The animal image dataset was originally for image classification as it contains 5,400 images of 90 different animals \cite[]{kaggle_animal}. For this project’s purpose, this dataset is ideal as it results in a diverse set of images with an equal distribution of the number of images per animal. 

The scenic image dataset from \cite{kaggle_scene} contains 4,319 images of a variety of landscapes that encompass a large breadth of colour palettes, which can impact how well the final model performs.

All the images from each dataset were extracted from their original folder organizations and relocated into a folder corresponding to their category. Due to the disparity in the size of the three datasets, each dataset was reduced to exactly 4200 images using Python’s “random.sample” function with the seed set to 42. 

The three sets of images were then named and converted to 256 x 256 pixel images using the resize method from PyTorch’s library of transformations. These ground truth images were then transferred into a folder corresponding to their category. Following this, the input images were created by taking the 256 x 256 pixel colour images from each folder and removing the RGB channels to create 256 x 256 pixel grayscale images to feed into the model. 

To create the training, validation and test sets, a ratio of 70:15:15 was chosen. ~write steps 

The final dataset is composed of 12,600 pairs of color and their corresponding grayscale images. Each category (human, animal, and scenic) contains 4,200 image pairs. There are 8,820 pairs in the training set, and the validation and test sets each contain 1,890 image pairs. An even distribution of each category was maintained in the training, validation and test sets. 

~add some graphs and visuals  

\subsection{Illustration}

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.9\linewidth]{Figs/architecture.png}
  \caption{Encoder–decoder colorization network with skip connections.}
  \label{fig:architecture}
\end{figure}

The top row is the encoder (Conv ×2 → MaxPool blocks) that downsamples the grayscale L channel from 256×256 to 32×32 while increasing feature depth (64 → 512).
The bottom row is the decoder (UpConv blocks) that upsamples back to full resolution and predicts the two chroma channels (a, b). Dashed arrows show the three skip connections that pass fine-grained features from encoder to decoder stages.

\subsection{Architecture}

We propose a convolutional encoder–decoder network for the colorization task, which is a common and effective choice for image-to-image translation problems. \cite{leatvanich2025image} The model will operate in the CIE Lab color space: the input is a grayscale image (the L channel), and the network is trained to predict the two chrominance channels (a and b) as output. After prediction, the input L channel and the output a,b channels are combined and converted back to an RGB image for visualization. Using the Lab space decouples intensity from color, which aligns with human vision and generally produces more realistic results than direct RGB prediction. \cite{leatvanich2025image}

The architecture itself is an encoder–decoder CNN resembling a simplified U-Net. The encoder consists of several convolutional layers (with ReLU activations and batch normalization) that gradually downsample the image, extracting higher-level features as the spatial size reduces. For example, we might start with a 3x3 conv layer with 64 filters, followed by another conv layer with 128 filters, each followed by a pooling (or stride-2 conv) to halve the spatial dimensions. After a few such layers, a bottleneck layer (e.g. a 3x3 conv with 256 filters) will capture abstract features of the grayscale input. The decoder then mirrors this process: it uses upsampling layers (e.g. Conv2DTranspose or nearest-neighbor upsampling) and additional conv layers to upsample the feature maps back to the original image resolution while predicting the a,b color channels. We plan to include skip connections between matching encoder and decoder levels (as in U-Net) so that fine-grained details from early layers (edges, textures) are directly passed to the corresponding decoder layers. These skip connections should help the network recover details and prevent blurry outputs by combining low-level and high-level features. \cite{leatvanich2025image}

To balance performance and simplicity, we will leverage transfer learning for the encoder. In particular, we are considering initializing the encoder with a pretrained backbone like the first few layers of ResNet-18 or VGG16 (trained on ImageNet). This gives the model a head-start in recognizing semantic features—useful given our dataset of human and animal images, where recognizing objects like faces, fur, or backgrounds can inform color choices. \cite{olah2022lettherebecolor} Using a pre-trained encoder should enable the network to learn meaningful colorization with fewer training examples and epochs. \cite{olah2022lettherebecolor} The decoder and any additional layers will be initialized randomly and learned from scratch. We will train the network end-to-end to minimize a pixel-wise loss between the predicted and ground-truth color channels (e.g. mean squared error in Lab space).

It’s worth noting that some prior works frame colorization as a classification problem over discrete color bins to better capture the ambiguous nature of predicting color. For instance, Zhang et al. (2016) predict a probability distribution over possible a,b values for each pixel instead of regressing exact values. \cite{olah2022lettherebecolor} While such techniques (often coupled with class re-balancing or perceptual losses) can produce vibrant results, they add complexity beyond the scope of a course project. In our architecture, we favor a straightforward regression approach – the network directly outputs continuous a and b values – which is simpler to implement and sufficient for plausible colorization.

\subsection{Baseline Model}

As a baseline, we will start with a very simple colorization approach against which to compare our full model. One trivial baseline is to output the input grayscale image as-is in color (i.e. replicate the L channel into RGB), which yields a degenerate colorization. This sets a minimum benchmark – any learning model should outperform simply producing a monochrome image.

For a more meaningful baseline, we will implement a shallow convolutional network inspired by basic encoder–decoder examples. \cite{leatvanich2025image} This baseline model will take the grayscale L channel as input and produce a,b chrominance channels, but it will have only a small fraction of the capacity of our main architecture. For instance, the baseline could use just two convolutional layers for encoding (e.g. 64 filters and 128 filters) followed by a single upsampling decoder to reconstruct the output colors. Concretely, the image might pass through Conv2D(64) and Conv2D(128) layers (with ReLU), then a pooling layer to downsample; at that point a minimal “bottleneck” conv layer can be applied, and finally an upsampling (Conv2DTranspose) step generates the 2-channel output map. \cite{leatvanich2025image} This simplistic model does not incorporate any skip connections or pretrained knowledge, and it has far fewer parameters than the proposed full model. We expect its predictions will capture only basic correlations (for example, mapping lightness to a bland average color) and often look desaturated or unrealistic. \cite{rosebrock2019bwcolorization}

Such a baseline provides a useful point of comparison: it represents what a rudimentary learning method can achieve without much capacity or context. By evaluating our advanced model against this baseline, we can quantify the gains from using a deeper architecture and more sophisticated design. If the baseline’s output is dull or mostly grayish (as often seen in naive colorization attempts), whereas our full model produces more vibrant and context-appropriate colors, it will demonstrate the effectiveness of the chosen architecture. \cite{rosebrock2019bwcolorization}

\section{Ethical Considerations}
\label{ethical}

The dataset being used is public, so there are no copyright or consent issues. However, the dataset may contain racial or demographic imbalances, which could cause the model 
to generalize poorly or be biased towards specific skin tones. This may result in racially inaccurate or culturally insensitive outputs. A similar behaviour may be observed with 
animals, where a lack of diversity in breeds or fur colours in the dataset can result in misleading results. If the outputs produced by the model are used in educational contexts 
or in breed identification, they can contribute to misinformation. Furthermore, since the model results are plausible but cannot be verified, there is a risk that users may 
overtrust the outputs in sensitive contexts.

\section{Project Plan}
\label{project_plan}

Our high-level project planning has been completed using a Gantt chart seen below. It contains many of the detailed tasks we have completed and will complete in the future. This
easily shows who is responsible for each task, the time it will take to complete, and the dependencies between tasks. Generally, our internal deadlines have been set at least a 
few days prior to the official deadlines to ensure we have time to review our work and make any necessary changes. In the event that a new member joins the team, they can easily
refer to this chart to see what responsibilites they can take on and what tasks are still available. Our Gantt chart can be found in Appendix A.

\subsection{Team Communication \& Coordination}

We intend to use Discord as our primary communication platorm, Google Drive for file sharing (Colab notebooks, datasets, etc.), and GitHub for version control. To ensure we do 
not overwrite each other's work, we will use branches for each feature or task, and merge them into the main branch once they are complete. We will also have weekly meetings to
discuss our progress, any issues we are facing, and plan for any action items for the subsequent meeting. Currently, we intend to meet on Saturday mornings at 8:00 AM EST as some
of our team members are in different time zones.

\section{Risk Register}
\label{risk_register}

\label{last_page}

\bibliography{APS360_ref}
\bibliographystyle{iclr2022_conference}

\section{Appendix A: Gantt Chart}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.58\linewidth]{Figs/gant-chart-full.jpg}
  \label{fig:gantt_chart}
\end{figure}
\end{document}
