\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
%\newcommand{\apsname}{Project Proposal}
\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{40}

\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{url}
\usepackage{graphicx}

%######## APS360: Put your project Title here
\title{Image Colourization via Convolutional \\
Neural Networks and Deep Learning}

%######## APS360: Put your names, student IDs and Emails here
\author{Youssef Fikry  \\
Student\# 1005678901\\
\texttt{youssef.fikry@mail.utoronto.ca} \\
\And Harkirpa Kaur  \\
Student\# 1011242479 \\
\texttt{harkirpa.kaur@mail.utoronto.ca} \\
\AND
Peter Leong \\
Student\# 1005678901 \\
\texttt{peter.leong@mail.utoronto.ca} \\
\And
Thulasi Thavarajah \\
Student\# 10115358424 \\
\texttt{t.thavarajah@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
This project addresses the challenge of automated colourization for 256$\times$256 grayscale images using a dataset of 12,600 image pairs, balanced across human subjects, 
animals, and natural scenery. We frame colourization as a supervised learning problem in the CIELAB colour space, where a model predicts chrominance channels ($a^*$, $b^*$) 
from the luminance channel ($L^*$). A shallow convolutional neural network (CNN) provides the baseline performance, while our primary solution employs a deeper convolutional encoder-decoder architecture. This 
design captures high-level semantic features and spatial context, addressing limitations of shallow networks in perceptual realism. All source code, datasets, and results 
are publicly available \href{https://drive.google.com/drive/folders/1cV1NhlQ8UTk_CgJdwhqeRu0z5xE85ZsI?usp=sharing}{here}. 
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}


\section{Brief Project Description}

The invention of photography provided the technology to capture a moment in time. However, for most of photographic history, the process of obtaining coloured images eluded photographers \citep{scienceandmediamuseum2020}. As a result, much of historic photography is grayscale; lacking the visual richness found in modern photography and is inaccessible to individuals with vision impairments \citep{vogelsang2024impact}. This project aims to leverage deep learning to automate the colorization of grayscale images, thereby aiding in the revitalization of grayscale photographs. Furthermore, image colorization technology assists archivists and museums in restoring lost visual information and has applications in the media, medical and geospatial industries. 

Traditional image colorization methods involve manual labour that is costly and time-consuming \citep{farella2022}. On the other hand, deep learning approaches automate the colorization process and supplant traditional techniques \citep{farella2022}. In machine learning, colorization is defined as a model taking a black-and-white image as an input, and outputting its coloured counterpart \citep{lettherebecolor}. From a machine learning perspective, deep convolutional neural networks are best suited for this task as they can extract and learn features such as colors, patterns and shapes in images and affiliate them with object classes \citep{deepcnn}. These characteristics aid CNNs in excelling at object classificition tasks, as well as image colorization \citep{deepcnn}. 

//insert image here. 

\section{Individual Contributions \& Responsibilities}
The team is collaborating effectively by dividing the work based on technical strengths and ensuring transparency through regular updates. Thulasi has led the data processing pipeline, 
handling cleanup, data splitting, and restructuring of the dataset for human, animal, and scenic image categories. Peter developed and trained the baseline model using a shallow CNN architecture 
and is responsible for model training, as he has access to a better GPU. Youssef and Kirpa are responsible for model optimization and implementation; however, all modelling decisions are made collaboratively. 
The current model is producing brown-hued images, and discussion is underway to find possible fixes. The team communicates through a dedicated Discord server, while all code is written on Colab and organized 
in a shared Google Drive, along with meeting minutes and the Gantt chart. All project deliverables are stored in a public GitHub repository with separate branches for each team member to avoid merge conflicts. 
The meeting leader's role alternates weekly, and the Gantt chart is updated during each meeting to track progress. A Gantt chart showing the updated project plan and work distribution can be found in Appendix X.

Redundancies are in place to minimize project risks. Currently, Peter runs the model training due to GPU access, but both Youssef and Harkirpa have compatible hardware to take over if needed. All code is documented 
with extensive comments and stored centrally in the shared drive to ensure that any member can interpret and modify it if someone becomes unavailable.

\section{Notable Contribution}

\subsection{Data Processing}

\subsection{Baseline Model}

To establish a performance benchmark for our colorization task, we implemented a shallow convolutional neural network called \texttt{ShallowColorizationNet}. This model uses a simple 
encoder-bottleneck-decoder architecture and does not incorporate adversarial training. Its purpose is to test the feasibility of learning a direct mapping from grayscale luminance (\textit{L}) 
input to the chrominance (\textit{ab}) channels in the CIELAB color space.

\subsubsection{Model Architecture}

The architecture of the baseline model is as follows:
\begin{itemize}
    \item \textbf{Encoder:} Two convolutional layers with ReLU activation followed by max pooling, reducing spatial dimensions while increasing channel depth.
    \item \textbf{Bottleneck:} A single convolutional layer that expands the feature representation.
    \item \textbf{Decoder:} A transposed convolution to restore spatial resolution and a final convolutional layer outputting two channels (for \textit{a} and \textit{b}), with a $\tanh$ 
    activation to constrain values between $[-1, 1]$.
\end{itemize}

\subsubsection{Training and Comparison Approach}

The baseline model was trained using the L1 loss between predicted and ground truth \textit{ab} channels. This model was compared to our primary neural network—a conditional 
GAN model that includes a discriminator to encourage more realistic colorizations—based on both quantitative and qualitative results.

\subsubsection{Quantitative and Qualitative Results}

\textbf{Quantitative:} The baseline model was trained for 5 epochs using L1 loss. The generator's loss remained stable around \textbf{1.5647}, while the discriminator's loss 
converged to approximately \textbf{0.7074}. These consistent values suggest that the baseline network was able to learn a basic mapping from grayscale to chrominance, but likely 
lacked the capacity or incentive to produce high-fidelity or diverse outputs. These values serve as reference points for evaluating improvements made by our primary GAN-based model.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figs/baseline_learning_curve.jpg}
    \caption{Generator and discriminator loss values over 5 epochs of training. The architecture will be investigated further during the final phase of the project to determine why 
    the loss values fluctuate by such small amounts.}

    \label{fig:baseline_outputs}
\end{figure}

\textbf{Qualitative:} The baseline model produced smooth but somewhat desaturated colorizations. It tended to predict average colors in ambiguous regions, resulting in low color 
diversity. See Figure~\ref{fig:baseline_outputs} for representative examples.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figs/baseline_output.png}
    \caption{Sample output from the baseline model on grayscale input images.}
    \label{fig:baseline_outputs}
\end{figure}

\subsubsection{Challenges}

The primary challenge encountered during development was the tendency of the baseline model to learn overly conservative color predictions. Without a discriminator or explicit 
diversity loss, the network favored colorizations close to the dataset mean. Additionally, selecting the appropriate normalization strategy for LAB space and constraining output to 
valid chrominance ranges required careful tuning.

\subsubsection{Feasibility Assessment}

Despite its simplicity, the baseline model demonstrated that the colorization task is learnable with a low-capacity network, although the outputs lacked the vividness and fidelity 
achieved by our primary GAN-based model. The baseline thus served its role in confirming the viability of end-to-end colorization from grayscale input and establishing a benchmark 
for model improvement.

\subsection{Primary Model}

Our colouriser combines a lightweight ResNet-18 encoder with a U-Net decoder. This design keeps global scene context (from the pretrained encoder) while preserving fine edges through skip connections, all within a 15 M-parameter budget suitable for Google Colab.

\subsubsection{Architecture}

The encoder copies the first four blocks of ResNet-18, pretrained on ImageNet, but the opening convolution is modified to ingest a single-channel L image. Feature maps are therefore produced at 256×256, 128×128, 64×64 and 32×32.
A symmetric decoder upsamples in three steps (transpose-conv → 3 × 3 conv × 2, BatchNorm, ReLU). Skip links inject the corresponding encoder features to keep edges crisp. A final 3 × 3 layer with tanh activation outputs the a and b chroma channels. Total capacity is ~15 M parameters, comfortably below Colab’s memory ceiling. The whole network sits under 25 MB on disk, so checkpoints save quickly.

\subsubsection{Training Progress}

We train with AdamW (lr = 3 × $10^{-4}$, batch = 32) on the cleaned 12.6 k-image set, and the learning rate decays each epoch via cosine annealing. Five epochs on a pro A100 GPU took just under 20 minutes. Validation L1 dropped from 0.07 to 0.0064, and SSIM climbed from 0.58 to 0.74. Qualitatively the model recovers plausible sky blues and fur textures that the two-layer baseline could not capture. The model also learns to colorize human skin tones, but it struggles with complex scenes like forests and crowds, where it tends to produce muddy colors.

\subsubsection{Challenges}

On some photos the network over-uses warm hues, giving a slight brown cast. Early inspection shows that brown-dominant scenes are over-represented in the training set and the loss does not penalise colour bias. We plan to rebalance LAB histograms per batch and add a small perceptual term to discourage monotone outputs.

\subsubsection{Feasibility Assessment}

After five epochs the model already lifts SSIM from 0.58 (baseline) to 0.74, showing it can learn credible colour cues. Remaining errors are mostly a warm “brown” bias. Because these mistakes are consistent, modest adjustments should yield clear gains. We plan to (i) rebalance training batches so cool and warm tones appear equally, and (ii) add a light perceptual loss to discourage over-use of one hue. These changes require no extra parameters or memory. With a 20-epoch run and a cosine learning-rate schedule, we expect further improvement toward an SSIM of 0.80 while staying within course limits.

\section{Data Processing}

\section{Baseline Model}

\section{Primary Model}
\label{last_page}

\newpage
\bibliographystyle{iclr2022_conference}
\bibliography{APS360_Report_ref}

\end{document}
