\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{40}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

%######## APS360: Put your project Title here
\title{Image Colourization via Convolutional \\
Neural Networks and Deep Learning}


%######## APS360: Put your names, student IDs and Emails here
\author{Youssef Fikry  \\
Student\# 1005678901\\
\texttt{youssef.fikry@mail.utoronto.ca} \\
\And
Harkirpa Kaur  \\
Student\# 1011242479 \\
\texttt{harkirpa.kaur@mail.utoronto.ca} \\
\AND
Peter Leong \\
Student\# 1005678901 \\
\texttt{peter.leong.utoronto.ca} \\
\And
Thulasi Thavarajah \\
Student\# 10115358424 \\
\texttt{t.thavarajah@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
This template should be used for all your project related reports in APS360 course. -- Write an abstract for your project here. Please review the \textbf{ First Course Tutorial} for a quick start
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}



\section{Introduction}

While the colour photography processes first emerged in the 1890s, colour photography did not become widely accessible until the 1970s \cite{scienceandmediamuseum2020}. As a result, 
most historic photographs are black and white and lack the visual richness that modern viewers are accustomed to. In addition, individuals who have their cataracts 
removed as a part of vision restoration processes have shown to struggle with identifying objects in black and white images \cite{vogelsang2024impact}, making most historic photographs 
inaccessible to them. This project aims to use deep learning to automatically colorize black and white images, with the goal of restoring visual information and 
making historical imagery more accessible for all audiences. Traditional, non deep-learning based colorization methods often produce desaturated results and rely 
heavily on human guidance \cite{cheng2016deepcolorization}, making them non-scalable. Conversely, deep networks such as CNNs effectively capture spatial and semantic features and produce 
realistic colourized images without user interaction \cite{zhang2016colorful}, making deep learning an ideal approach for image colorization.

\subsection{Background \& Related Work}

The challenge of image colourization has been approached in a wide variety of ways. Even within solutions involving deep learning
techniques, there numerous unique design choices. One grouping method \cite{zeger2021grayscale} results in five categories: simple colourization neural networks,
user-guided colourization neural networks, diverse colourization neural networks, multi-path colourization neural networks, and examplar-based
neural networks. 

Simple colourization neural networks use feedforward CNNs to map grayscale inputs to colour outputs. One of the foremost solutions proposed by
Zhang et al. \cite{zhang2016colorful} used a fully convolutional network to predict the a and b channels of the CIELAB colour space from grayscale images. Their 
architecture is composed of several convulational layers, each followed by a ReLU activation function and a batch normailization layer.

User-guided colourization neural networks use user input to guide the colourization process. One such solution \cite{zhang2017real} uses a
fully convolutional network to predict the a and b channels of the CIELAB colour space from grayscale images, but also takes user input in the form of
user-provided colour scribbles. The network is trained to minimize the difference between the predicted and user-provided colours, allowing it to
learn to colourize images in a way that is consistent with the user's input. 

Diverse colourization networks produce multiple colourization outputs for a given grayscale input. One such solution \cite{Vitoria2020ChromaGAN} uses
a generative adversarial network (GAN) to produce multiple colourization outputs for a given grayscale input. The GAN is trained to minimize the difference
between the predicted and ground truth colours, allowing it to learn to produce diverse colourization outputs. 

Multi-path colourization neural networks differentiate features at different scales. Iizuka et al. \cite{Iizuka2016Colourization} proposed a multi-path
colourization neural network that uses multiple convolutional layers to extract features at different scales. The network is trained to minimize the difference
between the predicted and ground truth colours, allowing it to learn to produce colourization outputs that are consistent with the features at different scales. 

Exemplar-based neural networks use a set of exemplar images to guide the colourization process. In Su et al \cite{su2020instanceawareimagecolorization}, example images are
used to trasnfer the colour to the target image. Each instance is ouput to two different colourization networks which fuse to yield the final result. This group of 
solutions is easier to implement, as learning to colourize instances is significantly easier than learning to colourize an entire image. 

\section{Methodology}
\label{methodology}

\subsection{Data Processing}

The project's image dataset was compiled using individual datasets from Kaggle.com, an online platform that provides access to real-world datasets and a community for data scientists. To train a model that can generalize to a broader range of images, the final dataset for this project is comprised of three categories: human, animal, and scenic. All datasets chosen have a public domain license.  

The human image dataset was chosen because its original purpose is for human detection \cite[]{kaggle_human}. As a result, the dataset contains a diverse range of 17,300 images of people in different environments. 

The animal image dataset was originally for image classification as it contains 5,400 images of 90 different animals \cite[]{kaggle_animal}. For this project’s purpose, this dataset is ideal as it results in a diverse set of images with an equal distribution of the number of images per animal. 

The scenic image dataset from \cite{kaggle_scene} contains 4,319 images of a variety of landscapes that encompass a large breadth of colour palettes, which can impact how well the final model performs.

All the images from each dataset were extracted from their original folder organizations and relocated into a folder corresponding to their category. Due to the disparity in the size of the three datasets, each dataset was reduced to exactly 4200 images using Python’s “random.sample” function with the seed set to 42. 

The three sets of images were then named and converted to 256 x 256 pixel images using the resize method from PyTorch’s library of transformations. These ground truth images were then transferred into a folder corresponding to their category. Following this, the input images were created by taking the 256 x 256 pixel colour images from each folder and removing the RGB channels to create 256 x 256 pixel grayscale images to feed into the model. 

To create the training, validation and test sets, a ratio of 70:15:15 was chosen. ~write steps 

The final dataset is composed of 12,600 pairs of color and their corresponding grayscale images. Each category (human, animal, and scenic) contains 4,200 image pairs. There are 8,820 pairs in the training set, and the validation and test sets each contain 1,890 image pairs. An even distribution of each category was maintained in the training, validation and test sets. 

~add some graphs and visuals  


\subsection{Architecture}

\subsection{Baseline Model}

\section{Ethical Considerations}
\label{ethical}

The dataset being used is public, so there are no copyright or consent issues. However, the dataset may contain racial or demographic imbalances, which could cause the model 
to generalize poorly or be biased towards specific skin tones. This may result in racially inaccurate or culturally insensitive outputs. A similar behaviour may be observed with 
animals, where a lack of diversity in breeds or fur colours in the dataset can result in misleading results. If the outputs produced by the model are used in educational contexts 
or in breed identification, they can contribute to misinformation. Furthermore, since the model results are plausible but cannot be verified, there is a risk that users may 
overtrust the outputs in sensitive contexts.

\section{Project Plan}
\label{project_plan}

Our high-level project planning has been completed using a Gant chart seen below. It contains many of the detailed tasks we have completed and will complete in the future. This
easily shows who is responsible for each task, the time it will take to complete, and the dependencies between tasks. Generally, our internal deadlines have been set at least a 
few days prior to the official deadlines to ensure we have time to review our work and make any necessary changes. In the event that a new member joins the team, they can easily
refer to this chart to see what responsibilites they can take on and what tasks are still available.

\subsection{Team Communication \& Coordination}

We intend to use Discord as our primiary communication platorm, Google Drive for file sharing (Colab notebooks, datasets, etc.), and GitHub for version control. To ensure we do 
not overwrite each other's work, we will use branches for each feature or task, and merge them into the main branch once they are complete. We will also have weekly meetings to
discuss our progress, any issues we are facing, and plan for any action items for the subsequent meeting. Currently, we intend to meet on Saturday mornings at 8:00 AM EST as some
of our team members are in different time zones.

\section{Risk Register}
\label{risk_register}

\label{last_page}

\bibliography{APS360_ref}
\bibliographystyle{iclr2022_conference}

\end{document}
