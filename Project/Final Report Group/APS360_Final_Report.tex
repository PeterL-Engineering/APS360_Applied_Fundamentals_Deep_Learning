\documentclass{article} % For LaTex2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
%\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{40}

\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{url}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{float}

%######## APS360: Put your project Title here
\title{Image Colourization via Convolutional \\
Neural Networks and Deep Learning}

%######## APS360: Put your names, student IDs and Emails here
\author{Youssef Fikry  \\
Student\# 1006682626\\
\texttt{youssef.fikry@mail.utoronto.ca} \\
\And Harkirpa Kaur  \\
Student\# 1011242479 \\
\texttt{harkirpa.kaur@mail.utoronto.ca} \\
\AND
Peter Leong \\
Student\# 1010892955 \\
\texttt{peter.leong@mail.utoronto.ca} \\
\And
Thulasi Thavarajah \\
Student\# 10115358424 \\
\texttt{t.thavarajah@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTex{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTex{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIx}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
This project addresses the challenge of automated colourization for 256$\times$256 grayscale images using a dataset of 12,600 image pairs, balanced across human subjects, 
animals, and natural scenery. We frame colourization as a supervised learning problem in the CIELAB colour space, where a model predicts chrominance channels ($a^*$, $b^*$) 
from the luminance channel ($L^*$). A shallow convolutional neural network (CNN) provides the baseline performance, while our primary solution employs a deeper convolutional 
encoder-decoder architecture. This design captures high-level semantic features and spatial context, addressing limitations of shallow networks in perceptual realism. All 
source code, datasets, and results are publicly available \href{https://drive.google.com/drive/folders/1cV1NhlQ8UTk_CgJdwhqeRu0z5xE85ZsI?usp=sharing}{here}. 
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}


\section{Introduction}

While colour photography processes first emerged in the 1890s, colour photography did not become widely accessible until the 1970s \citep{scienceandmediamuseum2020}. 
Consequently, most historical photographs remain in black and white, lacking the visual richness that modern viewers are accustomed to. Moreover, individuals who undergo cataract 
removal as part of vision restoration procedures often struggle to interpret grayscale images, rendering many historical photographs inaccessible to them \citet{vogelsang2024impact}. 
This project aims to leverage deep learning to automatically colourize black and white images, with the goal of restoring visual information and improving accessibility for all audiences. 
Traditional, non-deep learning colourization methods tend to produce desaturated results and require extensive human input, limiting their scalability \citep{cheng2016deepcolorization}. 
In contrast, deep neural networks such as convolutional neural networks (CNNs) can effectively learn spatial and semantic features, enabling realistic colourization without user 
intervention \citep{zhang2016colorful}. This makes deep learning a promising and scalable solution for image colourization.

\section{Background \& Related Work}

The challenge of image colourization has been addressed through a range of methods, particularly within deep learning. Even among deep learning-based solutions, researchers 
have proposed a variety of architectures, which can be broadly categorized into five groups: simple colourization neural networks, user-guided colourization networks, diverse 
colourization networks, multi-path networks, and exemplar-based approaches \citep{zeger2021grayscale}.

Simple colourization neural networks use feedforward convolutional neural networks (CNNs) to directly map grayscale inputs to colour outputs. One of the most influential examples 
is the work by \citet{zhang2016colorful}, who proposed a fully convolutional network that predicts the \textit{a} and \textit{b} channels in the CIELAB colour space. Their 
architecture comprises several convolutional layers, each followed by ReLU activations and batch normalization, and is trained as a classification task over quantized ab values, 
producing more vivid outputs than regression-based methods.

User-guided colourization networks incorporate human input to guide the colourization process. \citet{zhang2017real} extended their earlier work by accepting user-provided colour 
“scribbles” as input alongside the grayscale image. The network learns to propagate these hints across the image while minimizing differences from the target colour, allowing 
interactive and controllable colourization.

Diverse colourization networks aim to generate multiple plausible colourizations for a single grayscale input. For instance, \citet{Vitoria2020ChromaGAN} used a generative adversarial 
network (GAN) to produce diverse outputs by learning a conditional distribution over colourizations. This approach addresses the inherent ambiguity in mapping grayscale to colour.

Multi-path colourization networks extract features at multiple spatial resolutions to improve accuracy and context-awareness. \citet{Iizuka2016Colourization} proposed a model with both 
global and local feature pathways, enabling the network to learn both scene-level semantics and fine-grained textures. This structure helps ensure coherent colourization across different 
image regions.

Exemplar-based colourization networks transfer colour information from reference images to the target. In \citet{su2020instanceawareimagecolorization}, instance segmentation is used 
to match regions between the target and exemplars, and two separate colourization networks process this information before merging their outputs. This instance-level guidance simplifies 
the task compared to end-to-end full-image colourization and enhances accuracy in semantically similar scenes.

\section{Data Processing}

 The project's image dataset was compiled using publicly available datasets on Kaggle.com, an online platform that provides access to real-world datasets and a community for data 
 scientists \citep[]{kaggle}. To train a model that can generalize to a broad range of images, the final dataset for this project includes three categories: human, animal, and scenic. 
 All selected datasets are licensed for public domain use. 

\subsection{Repurposing Online Datasets}

The human image dataset, originally intended for human detection, contains a diverse range of 17,300 images of people in different environments \citep[]{kaggle_human}. Furthermore, 
the animal image dataset, initially developed for image classification contains 5,400 images of 90 different animals \citep[]{kaggle_animal}. For this project's purpose, this dataset 
is ideal as it encompasses a diverse set of images with an equal distribution of each animal. Additionally, the scenic image dataset from \citet{kaggle_scene} contains 4,319 images of 
a variety of landscapes spanning a large breadth of colour palettes, potentially influencing the robustness of the final model.

\subsection{Cleaning Up The Datasets}
The team extracted all the images from each dataset and relocated them into folders corresponding to their category (human/animal/scenic). Due to the disparity in the size of the 
three datasets, each dataset was reduced to exactly 4200 images using Python's \verb|random.sample| function with the seed set to 42. The images were then renamed in accordance to 
their respective categories (ex. human\_0001.jpg). 

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.65\linewidth]{Figs/Data Example.png}
  \caption{Sample Data Pair from Training Set}
  \label{fig:data_example}
\end{figure}

\subsection{Formatting the Data}

This project requires a unique dataset with black-and-white images paired with their coloured counterparts as the ground truths. To format the cleaned up data and create this dataset, 
PyTorch's \verb|torchvision.transforms| library was utiltized. The team first converted the original images into 256 x 256 pixel ground truth images using the \verb|Resize| transform 
and sorted them according to their category. Following this, \verb|Grayscale| transform with a output channel of 1 was used to convert the 256 x 256 pixel colour images to 256 x 256 
pixel grayscale images to feed into the model. 

\subsection{Splitting Dataset Into Training, Validation and Test Sets}

To create the training, validation and test sets, a ratio of 70:15:15 was chosen. The first 2940 images of each of dataset categories (human/animal/scene) were selected for the training 
set. The remaining images were split evenly to create the validation and testing datasets. 

\subsection{The Final Dataset}

The final dataset is composed of 12,600 pairs of colour and their corresponding grayscale images. Each category (human, animal, and scenic) contains 4,200 image pairs and an even 
distribution of each category was maintained in the training, validation and test sets.

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.65\linewidth]{Figs/dataset1.png}
  \caption{Training, Validation and Test Set Distribution of 12600 Image Pairs}
  \label{fig:dataset}
\end{figure}

\section{Illustration}

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.9\linewidth]{Figs/architecture.png}
  \caption{Encoder-decoder colourization network with skip connections.}
  \label{fig:architecture}
\end{figure}

The top row shows the encoder, composed of blocks with two convolutional layers followed by max pooling, which progressively downsample the grayscale L channel from 256x256 to 32x32 
while increasing feature depth from 64 to 512. The bottom row depicts the decoder, which uses upsampling blocks to restore full resolution and predict the two chrominance channels 
(a and b). Dashed arrows indicate the three skip connections that transfer fine-grained features from corresponding encoder layers to the decoder stages.

\section{Architecture}

Our primary model for colourization is a convolutional encoder-decoder network, a common architecture for image-to-image translation tasks \citep{leatvanich2025image}. Operating in 
the CIE Lab colour space, the model takes the L channel (lightness) as input and predicts the a and b chrominance channels. To ensure input-target consistency, we derive the L channel 
directly from the same RGB image used to generate the ground truth a,b channels. The predicted chrominance values are then combined with the L channel and converted back to RGB for 
visualization. This use of Lab space decouples intensity from colour, aligning with human perception and typically yielding more realistic results than RGB-based models \citep{leatvanich2025image}.

The architecture resembles a simplified U-Net. The encoder consists of convolutional layers with ReLU activations and batch normalization that progressively downsample the input while 
extracting high-level features. For example, the model starts with a 3×3 convolution producing 64 feature maps, followed by layers with increasing filter counts (e.g., 128), each halving 
spatial resolution. A bottleneck with 256 filters captures abstract representations. The decoder upsamples these back to the original resolution using transposed convolutions, predicting 
the a,b channels. Skip connections between encoder and decoder layers help preserve edge and texture details \citep{leatvanich2025image}.

We initialized the encoder with ResNet-18 weights pretrained on ImageNet, leveraging semantic features learned from large-scale datasets to better infer plausible colours for common 
objects \citep{olah2022lettherebecolor}. The decoder and output layers were trained from scratch.

Rather than formulating colourization as classification over discrete colour bins \citep{olah2022lettherebecolor}, we use a regression-based approach that directly predicts continuous 
a,b values. The model uses a linear output head without tanh activation to avoid chroma saturation and preserve the full colour range.

To encourage vibrant, diverse colourization and reduce bias toward brownish tones, we apply a weighted colour loss informed by a dataset-wide chrominance prior. We also use a robust 
(Huber) pixel loss, which better handles uncertainty and avoids bias toward low-chroma predictions. All augmentations were applied consistently to both input and target images, with 
vertical flips excluded to preserve semantic orientation. Finally, to reduce training time, prior computations were cached and updated once per epoch, reducing training from six hours to one.

\section{Baseline Model}
\label{baseline}

Our main baseline was a shallow encoder-decoder convolutional neural network, designed to be lightweight and fast to train. The model accepted the L channel of a grayscale image as 
input and predicted the a and b chrominance channels. The encoder consisted of two convolutional layers: the first mapped the input to 64 channels using a 3x3 3x3 kernel with padding 1, 
followed by a ReLU activation; the second expanded to 128 channels using the same kernel size and activation. A max pooling layer with a 2x22x2 kernel and stride 2 then reduced the 
spatial resolution by half. This was followed by a bottleneck layer with 256 filters and a ReLU activation, which further processed the compressed feature representation \citep{leatvanich2025image}.

The decoder upsampled the feature map using a transposed convolution that reduced the number of channels from 256 back to 128 while restoring the original resolution, followed by ReLU. 
A final convolutional layer projected to 2 output channels corresponding to the a and b values, and a Tanh activation was applied to constrain the outputs to the normalized Lab colour 
range of [-1,1][-1,1] \citep{rosebrock2019bwcolorization}.

We made no architectural or hyperparameter changes to this baseline throughout the project. It lacked skip connections, pretrained components, or deep 
feature extraction, and as expected, often produced desaturated or muted results. Nonetheless, it served as a clear and reproducible benchmark against which the improvements from our 
full model could be evaluated.

\section{Quantitative Results}
\label{quant_results}

\section{Qualitative Results}
\label{qual_results}

Our primary model consistently outperformed the baseline, particularly on landscape scenes where the network was able to recover vibrant skies, foliage, and water with relatively 
realistic hues. In contrast, the baseline produced dull or grayish outputs that lacked semantic colour cues. Figure~\ref{fig:landscape_comparison} shows this clear contrast: while 
the baseline yields low-saturation results, the primary model introduces plausible blue and green tones, suggesting the network successfully learned common associations between luminance 
and scene content.

Performance was less reliable on images of animals and humans, especially when the ground truth involved bright or saturated colours. In these cases, both the baseline and primary models 
tended to favour brownish or muted tones, likely due to the loss function penalizing deviations from average colours. This averaging effect is illustrated in Figure~\ref{fig:final-model-zebra},
where facial features and fur appear washed out or overly neutral.

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.9\linewidth]{Figs/final-model-example.png}
  \caption{Example output of a zebra image from the primary model. The model successfully captures the zebra's stripes and fur texture, producing a realistic colourization.}
  \label{fig:final-model-zebra}
\end{figure}

We also experimented with several alternate architectures: diverse colourization, quantized outputs using colour bins, multi-path networks, and exemplar-based models. Despite their 
conceptual appeal, these approaches often underperformed our original model. Figure~\ref{fig:colour-bins-landscape} displays the results from the colour bin model. The exemplar and multi-path models 
frequently introduced a purplish tint across diverse inputs, while the quantized and diverse variants still exhibited the same dulling behaviour due to similar averaging tendencies. 
These results highlight that added architectural complexity does not guarantee improved performance, and in some cases, may exacerbate artefacts or introduce new failure modes.

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.9\linewidth]{Figs/colour-bins-landscape.jpg}
  \caption{Example output of the colour bin model on a landscape image. The model attempts to predict colours using quantized bins, but the results are often muted and lack vibrancy.}
  \label{fig:colour-bins-landscape}
\end{figure}

\section{Evaluation on New Data}
\label{new_data}

\section{Discussion}
\label{discussion}

Our model generally performed well, especially on landscape images, where predicted colours often aligned well with the ground truth. These results suggest the model successfully
learned broad structural features such as skies, foliage, and terrain. However, performance declined on images featuring humans and animals, particularly when the ground truth
contained highly saturated or vibrant colours. In such cases, both the baseline and our primary model tended toward muted, brownish hues. This behaviour likely arises from the
pixel-wise loss, which encourages the model to average colours to minimize error—resulting in desaturated outputs that reduce loss but compromise visual realism.

To explore alternative strategies, we experimented with several advanced colourization approaches, including diverse colourization (generating multiple plausible outputs),
quantized prediction using discrete colour bins, multi-path architectures, and exemplar-based methods. Despite their conceptual promise, these alternatives failed to outperform
our initial primary model, both quantitatively and qualitatively. In particular, the multi-path and exemplar-based models often produced outputs with a noticeable purple hue, 
suggesting instability or poor alignment between reference features and target content. Other approaches encountered similar issues of colour averaging, with results again skewing 
brown or gray to minimize loss.

These findings were unexpected, although the alternative architectures seemed promising in theory, our initial model consistently performed better in both accuracy and visual quality. 
This shows that adding architectural complexity does not necessarily lead to better outcomes. In fact, some modifications introduced new problems—for example, the exemplar-based and 
multi-path models often produced a noticeable purple hue. Others, such as the quantized and diverse models, still struggled with the same tendency to average colours, leading to dull 
or brownish results. Overall, this highlighted the importance of a balanced and well-tuned architecture. Through this process, we learned that effective colourization depends not just 
on network design, but also on careful attention to loss behaviour, data distribution, and how these elements influence model outputs.

\section{Ethical Considerations}
\label{ethical}

The dataset used is publicly available, so there are no concerns related to copyright or consent. However, it may reflect racial or demographic imbalances, which could cause the model 
to produce inaccurate or biased results, particularly in how it represents different skin tones. These inaccuracies can lead to racially or culturally inappropriate colourizations. A 
similar issue may occur with animals: if the dataset lacks variety in breeds or fur colours, the model may produce misleading or unrealistic images. If such outputs are used in educational 
materials or for breed identification, they could spread incorrect information.

Since the model generates colourizations that look realistic but cannot be verified, there is also a risk that people may place too much trust in the results—especially in situations 
where accuracy matters. It is important to clearly communicate the model's limitations and treat its outputs with appropriate caution.

\label{last_page}

\newpage
\bibliographystyle{iclr2022_conference}
\bibliography{APS360_Final_Report_ref}


\end{document}
