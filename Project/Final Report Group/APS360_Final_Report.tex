\documentclass{article} % For LaTex2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
%\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{40}

\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{url}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{float}
\usepackage{tikz}

%######## APS360: Put your project Title here
\title{Image Colourization via Convolutional \\
Neural Networks and Deep Learning}

%######## APS360: Put your names, student IDs and Emails here
\author{Youssef Fikry  \\
Student\# 1006682626\\
\texttt{youssef.fikry@mail.utoronto.ca} \\
\And Harkirpa Kaur  \\
Student\# 1011242479 \\
\texttt{harkirpa.kaur@mail.utoronto.ca} \\
\AND
Peter Leong \\
Student\# 1010892955 \\
\texttt{peter.leong@mail.utoronto.ca} \\
\And
Thulasi Thavarajah \\
Student\# 1011358424 \\
\texttt{t.thavarajah@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTex{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTex{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIx}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
This project addresses the challenge of automated colourization for 256$\times$256 grayscale images using a dataset of 12,600 image pairs, balanced across human subjects, 
animals, and natural scenery. We frame colourization as a supervised learning problem in the CIELAB colour space, where a model predicts chrominance channels ($a^*$, $b^*$) 
from the luminance channel ($L^*$). A shallow convolutional neural network (CNN) provides the baseline performance, while our primary solution employs a deeper convolutional 
encoder-decoder architecture. This design captures high-level semantic features and spatial context, addressing limitations of shallow networks in perceptual realism.
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}


\section{Introduction}

While colour photography processes first emerged in the 1890s, colour photography did not become widely accessible until the 1970s \citep{scienceandmediamuseum2020}. 
Consequently, most historical photographs remain in black and white, lacking the visual richness that modern viewers are accustomed to. Moreover, individuals who undergo cataract 
removal as part of vision restoration procedures often struggle to interpret grayscale images, rendering many historical photographs inaccessible to them \citet{vogelsang2024impact}. 
This project aims to leverage deep learning to automatically colourize black and white images, with the goal of restoring visual information and improving accessibility for all audiences. 
Traditional, non-deep learning colourization methods tend to produce desaturated results and require extensive human input, limiting their scalability \citep{cheng2016deepcolorization}. 
In contrast, deep neural networks such as convolutional neural networks (CNNs) can effectively learn spatial and semantic features, enabling realistic colourization without user 
intervention \citep{zhang2016colorful}. This makes deep learning a promising and scalable solution for image colourization.

\section{Background \& Related Work}

The challenge of image colourization has been addressed through a range of methods, particularly within deep learning. Even among deep learning-based solutions, researchers 
have proposed a variety of architectures, which can be broadly categorized into five groups: simple colourization neural networks, user-guided colourization networks, diverse 
colourization networks, multi-path networks, and exemplar-based approaches \citep{zeger2021grayscale}.

Simple colourization neural networks use feedforward convolutional neural networks (CNNs) to directly map grayscale inputs to colour outputs. One of the most influential examples 
is the work by \citet{zhang2016colorful}, who proposed a fully convolutional network that predicts the \textit{a} and \textit{b} channels in the CIELAB colour space. Their 
architecture comprises several convolutional layers, each followed by ReLU activations and batch normalization, and is trained as a classification task over quantized ab values, 
producing more vivid outputs than regression-based methods.

User-guided colourization networks incorporate human input to guide the colourization process. \citet{zhang2017real} extended their earlier work by accepting user-provided colour 
“scribbles” as input alongside the grayscale image. The network learns to propagate these hints across the image while minimizing differences from the target colour, allowing 
interactive and controllable colourization.

Diverse colourization networks aim to generate multiple plausible colourizations for a single grayscale input. For instance, \citet{Vitoria2020ChromaGAN} used a generative adversarial 
network (GAN) to produce diverse outputs by learning a conditional distribution over colourizations. This approach addresses the inherent ambiguity in mapping grayscale to colour.

Multi-path colourization networks extract features at multiple spatial resolutions to improve accuracy and context-awareness. \citet{Iizuka2016Colourization} proposed a model with both 
global and local feature pathways, enabling the network to learn both scene-level semantics and fine-grained textures. This structure helps ensure coherent colourization across different 
image regions.

Exemplar-based colourization networks transfer colour information from reference images to the target. In \citet{su2020instanceawareimagecolorization}, instance segmentation is used 
to match regions between the target and exemplars, and two separate colourization networks process this information before merging their outputs. This instance-level guidance simplifies 
the task compared to end-to-end full-image colourization and enhances accuracy in semantically similar scenes.

\section{Data Processing}

 The project's image dataset was compiled using publicly available datasets on Kaggle.com, an online platform that provides access to real-world datasets and a community for data 
 scientists \citep[]{kaggle}. To train a model that can generalize to a broad range of images, the final dataset for this project includes three categories: human, animal, and scenic. 
 All selected datasets are licensed for public domain use. 

\subsection{Repurposing Online Datasets}

The human image dataset, originally intended for human detection, contains a diverse range of 17,300 images of people in different environments \citep[]{kaggle_human}. Furthermore, 
the animal image dataset, initially developed for image classification contains 5,400 images of 90 different animals \citep[]{kaggle_animal}. For this project's purpose, this dataset 
is ideal as it encompasses a diverse set of images with an equal distribution of each animal. Additionally, the scenic image dataset from \citet{kaggle_scene} contains 4,319 images of 
a variety of landscapes spanning a large breadth of colour palettes, potentially influencing the robustness of the final model.

\subsection{Cleaning Up The Datasets}
The team extracted all the images from each dataset and relocated them into folders corresponding to their category (human/animal/scenic). Due to the disparity in the size of the 
three datasets, each dataset was reduced to exactly 4200 images using Python's \verb|random.sample| function with the seed set to 42. The images were then renamed in accordance to 
their respective categories (ex. human\_0001.jpg). 

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=1\linewidth]{Figs/Data Example.png}
  \caption{Sample Data Pair from Training Set}
  \label{fig:data_example}
\end{figure}

\subsection{Formatting the Data}

This project requires a unique dataset with black-and-white images paired with their coloured counterparts as the ground truths. To format the cleaned up data and create this dataset, 
PyTorch's \verb|torchvision.transforms| library was utiltized. The team first converted the original images into 256 x 256 pixel ground truth images using the \verb|Resize| transform 
and sorted them according to their category. Subsequently, the \verb|Grayscale| transform (output channel = 1) converted the 256$\times$256 pixel colour images into grayscale for input to the model.

\subsection{Splitting Dataset Into Training, Validation and Test Sets}

The datasets were split into training, validation, and test sets in a 70:15:15 ratio. For each category (human, animal, scene), the first 2940 images formed the training set, with the rest
split evenly between validation and testing.

\subsection{The Final Dataset}

The final dataset contains 12{,}600 colour-grayscale image pairs, evenly divided into three categories (human, animal, scenic) with 4{,}200 pairs each. Category balance was maintained across 
the training, validation, and test sets, illustrated in Figure~\ref{fig:data_split}. 

\begin{figure}[h]
\centering
\begin{tikzpicture}
% Define colours
\definecolor{traincol}{RGB}{100,149,237}   % Cornflower blue
\definecolor{valcol}{RGB}{60,179,113}     % Medium sea green
\definecolor{testcol}{RGB}{255,20,0}      % Red

% Total width and height of bar
\newlength{\totalwidth}
\setlength{\totalwidth}{14cm}
\newlength{\barheight}
\setlength{\barheight}{1.5cm}

% Widths proportional to 70:15:15
\newlength{\trainwidth}
\setlength{\trainwidth}{0.6\totalwidth}
\newlength{\valwidth}
\setlength{\valwidth}{0.2\totalwidth}
\newlength{\testwidth}
\setlength{\testwidth}{0.2\totalwidth}

% Draw stacked bar
\draw[fill=traincol] (0,0) rectangle (\trainwidth,\barheight) node[midway,white]{Training 8820};
\draw[fill=valcol] (\trainwidth,0) rectangle (\trainwidth+\valwidth,\barheight) node[midway,white]{Validation 1890};
\draw[fill=testcol] (\trainwidth+\valwidth,0) rectangle (\totalwidth,\barheight) node[midway,white]{Test 1890};

% Outline
\draw (0,0) rectangle (\totalwidth,\barheight);
\end{tikzpicture}
\caption{Distribution of 12{,}600 image pairs across training, validation, and test sets.}
\label{fig:data_split}
\end{figure}

\section{Architecture}

Our primary colourization model is a convolutional encoder-decoder network, a common architecture for image-to-image translation \citep{leatvanich2025image}. Operating in the CIELAB colour 
space, the model takes the L channel (perceptual lightness) as input and predicts the a and b chrominance channels. To ensure input-target consistency, the L channel is derived directly 
from the same RGB image used to generate the ground truth a,b channels. The predicted chrominance is then combined with the L channel and converted back to RGB for visualization. Using 
LAB space decouples intensity from colour, aligns with human perception, and typically produces more realistic results than RGB-based models \citep{leatvanich2025image}.

The architecture is a simplified U-Net. The encoder uses convolutional layers with ReLU activations and batch normalization to progressively downsample the input while extracting high-level 
features. It begins with a 3$\times$3 convolution producing 64 feature maps, followed by layers with increasing filter counts (e.g., 128) that halve spatial resolution. A bottleneck with 256 
filters captures abstract representations. The decoder upsamples back to the original resolution using transposed convolutions to predict the a,b channels, with skip connections preserving 
edges and textures \citep{leatvanich2025image}.

We initialize the encoder with ResNet-18 weights pretrained on ImageNet, leveraging semantic features learned from large-scale datasets to better infer plausible colours for common objects 
\citep{olah2022lettherebecolor}. The decoder and output layers are trained from scratch.

Rather than classifying discrete colour bins \citep{olah2022lettherebecolor}, we use a regression-based approach that directly predicts continuous a,b values. The output head is linear without 
\verb|tanh| activation, avoiding chroma saturation and preserving the full colour range.

To promote vibrant, diverse colourization and reduce bias toward brownish tones, we use a weighted colour loss informed by a dataset-wide chrominance prior. A robust (Huber) pixel loss further 
improves performance by handling uncertainty and avoiding bias toward low-chroma predictions. All augmentations are applied consistently to input and target images, with vertical flips excluded 
to preserve semantic orientation. To shorten training, prior computations are cached and updated once per epoch, reducing runtime from six hours to one.

\section{Illustration}

The top row shows the encoder, consisting of blocks with two convolutional layers followed by max pooling, progressively downsampling the grayscale L channel from 256$\times$256 to 32$\times$32 
while increasing feature depth from 64 to 512. The bottom row shows the decoder, which uses upsampling blocks to restore full resolution and predict the two chrominance channels (a and b). Coloured 
arrows mark four skip connections that transfer fine-grained features from encoder layers to corresponding decoder stages.

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=1.1\linewidth]{Figs/architecture-diagram.png}
  \caption{Encoder-decoder colourization network with skip connections.}
  \label{fig:architecture}
\end{figure}

\section{Baseline Model}
\label{baseline}

Our baseline was a shallow encoder-decoder convolutional network, designed to be lightweight and fast to train. The model took the L channel of a grayscale image as input and predicted the a and b 
chrominance channels. The encoder had two convolutional layers: the first mapped the input to 64 channels using a 3$\times$3 kernel with padding 1 and ReLU activation, and the second expanded to 128 
channels with the same kernel and activation. A max-pooling layer with 2$\times$2 kernel and stride 2 halved the spatial resolution, followed by a bottleneck layer with 256 filters and ReLU activation 
to process the compressed features \citep{leatvanich2025image}.

The decoder upsampled the features using a transposed convolution that reduced channels from 256 to 128 while restoring resolution, followed by ReLU. A final convolution projected to 2 output channels 
for the a and b values, with Tanh activation to constrain outputs to the normalized LAB range [-1,1] \citep{rosebrock2019bwcolorization}.

No architectural or hyperparameter changes were made to this baseline. It lacked skip connections, pretrained components, or deep feature extraction, and often produced desaturated results. Nevertheless, 
it provided a clear, reproducible benchmark to evaluate improvements from our full model.

\section{Quantitative Results}
\label{quant_results}

We used a combination of smooth loss and colourfullness metric to calculate the loss values for our model. The colourfullness metric encouraged the model to use more saturated colours
instead of resorting to brown hues, which is an issue we previously encountered. 

Validation and training loss was documented after every epoch and the results were graphed, as shown in Figure~\ref{fig:loss_curve}. We observed some overfitting, as the final training loss was 0.004423, 
and the final validation loss was 0.007595.

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.9\linewidth]{Figs/loss_curve.png}
  \caption{Learning curve showing training and validation loss of model over 30 epochs.}
  \label{fig:loss_curve}
\end{figure}

\section{Qualitative Results}
\label{qual_results}

Our primary model consistently outperformed the baseline, particularly on landscape scenes where the network was able to recover vibrant skies, foliage, and water with relatively 
realistic hues. In contrast, the baseline produced dull or grayish outputs that lacked semantic colour cues. Figure~\ref{fig:landscape_comparison} shows this clear contrast: while 
the baseline yields low-saturation results, the primary model introduces plausible blue and green tones, suggesting the network successfully learned common associations between luminance 
and scene content.

Performance was less reliable on images of animals and humans, especially when the ground truth involved bright or saturated colours. In these cases, both the baseline and primary models 
tended to favour brownish or muted tones, likely due to the loss function penalizing deviations from average colours. This averaging effect is illustrated in Figure~\ref{fig:final-model-zebra},
where facial features and fur appear washed out or overly neutral.

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.9\linewidth]{Figs/final-model-example.png}
  \caption{Example output of a zebra image from the primary model. The model successfully captures the zebra's stripes and fur texture, producing a realistic colourization.}
  \label{fig:final-model-zebra}
\end{figure}

We also experimented with several alternate architectures: diverse colourization, quantized outputs using colour bins, multi-path networks, and exemplar-based models. Despite their 
conceptual appeal, these approaches often underperformed our original model. Figure~\ref{fig:colour-bins-landscape} displays the results from the colour bin model. The exemplar and multi-path models 
frequently introduced a purplish tint across diverse inputs, while the quantized and diverse variants still exhibited the same dulling behaviour due to similar averaging tendencies. 
These results highlight that added architectural complexity does not guarantee improved performance, and in some cases, may exacerbate artefacts or introduce new failure modes.

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.9\linewidth]{Figs/colour-bins-landscape.jpg}
  \caption{Example output of the colour bin model on a landscape image. The model attempts to predict colours using quantized bins, but the results are often muted and lack vibrancy.}
  \label{fig:colour-bins-landscape}
\end{figure}

\section{Evaluation on New Data}
\label{new_data}

Our model was evaluated on a separate test set of 1,890 images, which were not used during training or validation. The model performed well relative to the validation results. More explicitly, the model achieved 
a final test loss of 0.007870, which is slightly higher than the validation loss of 0.007595. This indicates that the model generalizes well to unseen data, maintaining its ability to produce realistic colourizations across diverse image categories. 
Included below in Figure\ref{fig:test-data-example} is an example of the model's performance on a test image, demonstrating its ability to capture the scene's structure and colour distribution effectively.

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.9\linewidth]{Figs/test_data_result_example.png}
  \caption{Example output of from the test dataset. The model captures most of the scene's structure and colour distribution, but struggles to produce vibrant blue hues for the water.}
  \label{fig:test-data-example}
\end{figure}

\section{Discussion}
\label{discussion}

Our model generally performed well, especially on landscape images, where predicted colours often aligned well with the ground truth. These results suggest the model successfully
learned broad structural features such as skies, foliage, and terrain. However, performance declined on images featuring humans and animals, particularly when the ground truth
contained highly saturated or vibrant colours. In such cases, both the baseline and our primary model tended toward muted, brownish hues. This behaviour likely arises from the
pixel-wise loss, which encourages the model to average colours to minimize error—resulting in desaturated outputs that reduce loss but compromise visual realism. Furthermore, 
from colour clustering performed on the dataset (see Figure 7), the most dominant hues were grays and browns; this may have contributed to the preliminary model's muted, brownish outputs. 

\begin{figure}[htbp]            % h=here, t=top, b=bottom, p=page float
  \centering
  \includegraphics[width=0.95\linewidth]{Figs/dom-colours.png}
  \caption{The dominant five colours in dataset from K-means colour clustering}
  \label{fig:dom-colours}
\end{figure}

To explore alternative strategies, we experimented with several advanced colourization approaches, including diverse colourization (generating multiple plausible outputs),
quantized prediction using discrete colour bins, multi-path architectures, and exemplar-based methods. Despite their conceptual promise, these alternatives failed to outperform
our initial primary model, both quantitatively and qualitatively. In particular, the multi-path and exemplar-based models often produced outputs with a noticeable purple hue, 
suggesting instability or poor alignment between reference features and target content. 
Other approaches encountered similar issues of colour averaging, with results again skewing 
brown or gray to minimize loss.

These findings were unexpected, although the alternative architectures seemed promising in theory, our initial model consistently performed better in both accuracy and visual quality. 
This shows that adding architectural complexity does not necessarily lead to better outcomes. In fact, some modifications introduced new problems—for example, the exemplar-based and 
multi-path models often produced a noticeable purple hue. Others, such as the quantized and diverse models, still struggled with the same tendency to average colours, leading to dull 
or brownish results. Overall, this highlighted the importance of a balanced and well-tuned architecture. Through this process, we learned that effective colourization depends not just 
on network design, but also on careful attention to loss behaviour, data distribution, and how these elements influence model outputs.

\section{Ethical Considerations}
\label{ethical}

The dataset is publicly available, so there are no copyright or consent concerns. However, representation bias is possible if certain groups are overrepresented, such as lighter-skinned 
individuals, specific animal breeds, or particular fur colours. In such cases, the model may produce unrealistic or culturally insensitive colourizations. While these issues have not appeared 
in our outputs, they remain a potential risk, especially if colourized images are used for educational or identification purposes.

Evaluation bias is also a concern. Without knowledge of the dataset's demographic distribution, we cannot guarantee uniform model performance across all categories, which may skew metrics 
and misrepresent effectiveness.

Finally, because the model generates plausible but unverifiable outputs, users may place undue trust in the results. It is therefore important to communicate these limitations clearly and 
avoid applying the model in contexts that require factual accuracy.

\label{last_page}

\newpage
\bibliographystyle{iclr2022_conference}
\bibliography{APS360_Final_Report_ref}
\end{document}